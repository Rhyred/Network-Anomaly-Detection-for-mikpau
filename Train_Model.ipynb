{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook Pelatihan Model Deteksi Anomali\n",
                "\n",
                "Notebook ini digunakan untuk melatih model deteksi anomali berdasarkan data lalu lintas yang telah dikumpulkan oleh `DdosApi`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Impor Library\n",
                "\n",
                "Sel pertama ini mengimpor semua library yang kita butuhkan, termasuk `AnomalyDetector` yang telah kita buat."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Libraries imported successfully!\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import json\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Pastikan Anda menjalankan notebook ini dari direktori 'netwok_monitoring_ai'\n",
                "from anomaly_detector import AnomalyDetector\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Muat Data Pelatihan\n",
                "\n",
                "Selanjutnya, kita akan memuat data lalu lintas normal yang telah dicatat oleh API. Pastikan path ke `traffic_records` sudah benar."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading normal traffic data from ../DdosApi/traffic_records/...\n",
                        "Successfully loaded 77 data records.\n"
                    ]
                }
            ],
            "source": [
                "def load_data_from_dir(dir_path):\n",
                "    all_data = []\n",
                "    if not os.path.exists(dir_path):\n",
                "        print(f\"Error: Directory not found at '{dir_path}'\")\n",
                "        return all_data\n",
                "        \n",
                "    for filename in os.listdir(dir_path):\n",
                "        if filename.endswith(\".json\"):\n",
                "            with open(os.path.join(dir_path, filename), 'r') as f:\n",
                "                all_data.append(json.load(f))\n",
                "    return all_data\n",
                "\n",
                "# Path ke folder tempat DdosApi menyimpan data\n",
                "# Path ini relatif dari direktori 'netwok_monitoring_ai'\n",
                "normal_data_path = '../DdosApi/traffic_records/'\n",
                "\n",
                "print(f\"Loading normal traffic data from {normal_data_path}...\")\n",
                "normal_data = load_data_from_dir(normal_data_path)\n",
                "\n",
                "if normal_data:\n",
                "    print(f\"Successfully loaded {len(normal_data)} data records.\")\n",
                "else:\n",
                "    print(\"No training data found. Please run the main application to collect data first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Inisialisasi Anomaly Detector\n",
                "\n",
                "Kita membuat instance dari kelas `AnomalyDetector`. Model yang dilatih akan disimpan di dalam folder `checkpoints/` di dalam direktori ini."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "AnomalyDetector initialized.\n"
                    ]
                }
            ],
            "source": [
                "detector = AnomalyDetector(model_path='./checkpoints/')\n",
                "print(\"AnomalyDetector initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Latih Model\n",
                "\n",
                "Ini adalah sel utama di mana proses pelatihan terjadi. Jalankan sel ini hanya jika Anda sudah memiliki cukup data normal.\n",
                "\n",
                "**Catatan:** Proses ini bisa memakan waktu beberapa menit tergantung pada jumlah data dan kekuatan PC Anda."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting model training...\n",
                        "Starting model training with PyTorch...\n",
                        "Processing 77 normal data points...\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Data preprocessing complete. 11 features generated.\n",
                        "Training PyTorch Autoencoder for 20 epochs...\n",
                        "Epoch [1/20], Loss: 0.2280\n",
                        "Epoch [2/20], Loss: 0.2152\n",
                        "Epoch [3/20], Loss: 0.2013\n",
                        "Epoch [4/20], Loss: 0.1857\n",
                        "Epoch [5/20], Loss: 0.1677\n",
                        "Epoch [6/20], Loss: 0.1473\n",
                        "Epoch [7/20], Loss: 0.1246\n",
                        "Epoch [8/20], Loss: 0.1005\n",
                        "Epoch [9/20], Loss: 0.0767\n",
                        "Epoch [10/20], Loss: 0.0551\n",
                        "Epoch [11/20], Loss: 0.0375\n",
                        "Epoch [12/20], Loss: 0.0247\n",
                        "Epoch [13/20], Loss: 0.0161\n",
                        "Epoch [14/20], Loss: 0.0107\n",
                        "Epoch [15/20], Loss: 0.0073\n",
                        "Epoch [16/20], Loss: 0.0053\n",
                        "Epoch [17/20], Loss: 0.0039\n",
                        "Epoch [18/20], Loss: 0.0031\n",
                        "Epoch [19/20], Loss: 0.0025\n",
                        "Epoch [20/20], Loss: 0.0021\n",
                        "Generating latent representations...\n",
                        "Generating synthetic anomalies for classifier training.\n",
                        "Training a Logistic Regression classifier...\n",
                        "Classifier accuracy on validation set: 94.87%\n",
                        "Saving PyTorch models to ./checkpoints/...\n",
                        "Models saved successfully.\n",
                        "Training complete and models saved.\n",
                        "--- Training Finished ---\n",
                        "{'status': 'success', 'message': 'Models trained with classifier accuracy: 94.87%'}\n"
                    ]
                }
            ],
            "source": [
                "if normal_data:\n",
                "    print(\"Starting model training...\")\n",
                "    # Kita bisa menyesuaikan epochs di sini\n",
                "    training_result = detector.train(list_of_normal_data=normal_data, epochs=20)\n",
                "    print(\"--- Training Finished ---\")\n",
                "    print(training_result)\n",
                "else:\n",
                "    print(\"Skipping training because no data was loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Selesai!\n",
                "\n",
                "Jika sel di atas berjalan tanpa error, model baru Anda (termasuk `autoencoder.pth`, `classifier.pkl`, dan `scaler.pkl`) telah disimpan di folder `checkpoints/`.\n",
                "\n",
                "Langkah selanjutnya adalah me-restart `DdosApi`. Secara otomatis ia akan memuat model yang baru dilatih ini dan mulai memberikan prediksi yang sesungguhnya."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
